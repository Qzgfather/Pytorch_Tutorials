{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06、自定义数据集\n",
    "光用API加载数据我觉得不太行，只能自己自娱自乐跑跑例子，前几节稍微提到过数据集的定义，这一节对如何定义数据集进行详细的分析和讲解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch有很高的灵活性，你完全可以自己从零开始，使用numpy构建数据集，比如我进行一个for循环读取数据然后使用模型进行计算，或者自己定义一个类啥的都是很简单的，但是为了方便我觉得还是遵循pytorch的建议来继承某些类来进行数据集生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1]), tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1]), tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1]), tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0]), tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0]), tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1]), tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1]), tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1]), tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1]), tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "#  首先我可以使用numpy新建一些数据，直接使用网络进行训练，为了简单我使用LeNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 假装图片\n",
    "images = []\n",
    "labels = []\n",
    "for i in range(10):\n",
    "    images.append(torch.rand(10, 1, 28, 28))\n",
    "    labels.append(torch.randint(0, 2, [10]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(     #input_size=(1*28*28)\n",
    "            nn.Conv2d(1, 6, 5, 1, 2), #padding=2保证输入输出尺寸相同\n",
    "            nn.ReLU(),      #input_size=(6*28*28)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#output_size=(6*14*14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),      #input_size=(16*10*10)\n",
    "            nn.MaxPool2d(2, 2)  #output_size=(16*5*5)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  LeNet()\n",
    "certion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.6814153790473938, avg_loss3.407076895236969\n",
      "loss:0.8256223797798157, avg_loss4.128111898899078\n",
      "loss:0.7207103967666626, avg_loss3.603551983833313\n",
      "loss:0.6993415355682373, avg_loss3.4967076778411865\n",
      "loss:0.6882819533348083, avg_loss3.4414097666740417\n",
      "loss:0.8258719444274902, avg_loss4.129359722137451\n",
      "loss:0.5788917541503906, avg_loss2.894458770751953\n",
      "loss:0.8512927889823914, avg_loss4.256463944911957\n",
      "loss:0.7554869651794434, avg_loss3.777434825897217\n",
      "loss:0.7351841926574707, avg_loss3.6759209632873535\n"
     ]
    }
   ],
   "source": [
    "for i,l in zip(images, labels):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(i)\n",
    "    loss = certion(out, l)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'loss:{loss.item()}, avg_loss{loss.item() * i.size(0)}')\n",
    "#     _, pre = out.max(1)\n",
    "#     acc = (pre == l).sum().item()\n",
    "#     print(acc)\n",
    "#     acc_ = acc / i.size(0)\n",
    "#     print(acc_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里看到你可以使用torch、numpy新建一些数据或者处理一些数据来进行训练还是很简单的，但是呢随着数据量的增长这样写入内存的方式好像有点不太行所以你需要一个迭代器类来处理，假如你的python比较厉害可以写一个可迭代的类来处理数据，比如像下面一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Mydata:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # 可以在这里进行简单处理\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "for i in Mydata(images[0]):\n",
    "    print(i.shape)\n",
    "    # 然后就可以在这进行训练啥的，按需生成绝不浪费\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们又意识到，一个一个数据训练好像效果不太好，而且浪费了计算机CPU，GPU的算力，能不能一次处理很多数据比如一次计算100张图片，显然是可以你可以自己写点代码实现这个功能，为了方便我们直接使用pytorch自带的数据生成器，来对数据进行批次获取。\n",
    "\n",
    "- trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,shuffle=True)\n",
    "\n",
    "使用torch.utils.data.DataLoader就可以对数据进行获取批次的操作其中第一个参数是传入数据生成器，是一个可迭代对象，然后第二个就是批次大小，shuffle是是否打乱数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Mydata:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # 可以在这里进行简单处理\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# 我们的images有10个数据我们试试 用API一次产生两个图片的批次\n",
    "data_ = Mydata(images[0])\n",
    "loader = torch.utils.data.DataLoader(data_, batch_size=2,shuffle=True)\n",
    "for i in loader:\n",
    "    print(i.shape)\n",
    "    break\n",
    "# 看到输出就是2,1,28,28了，挺好，按需生成数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们又看到上一节数据生成的时候的数据为什么可以transform，对数据进行处理，这里我们介绍最后一个方法，新建类继承torch.utils.data.Dataset类然后重写len和getitem方法即可，len是统计数据有多长的，改进一下我们数据类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "# 为了验证transform的作用，我就用numpy产生数据。\n",
    "data = np.random.rand(10, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class Mydata(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        super(Mydata).__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        # 可以在这里进行简单处理\n",
    "    def __getitem__(self, index):\n",
    "        # 进行一个判断，看看是不是需要做transform\n",
    "        sample = self.data[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(self.data[index])\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "# 我们的images有10个数据我们试试 用API一次产生两个图片的批次\n",
    "data_ = Mydata(data, transform=transform)\n",
    "loader = torch.utils.data.DataLoader(data_, batch_size=2,shuffle=True)\n",
    "for i in loader:\n",
    "    print(type(i))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们生成的是numpy数据 但是我们调用 了transform方法将其转换为tensor格式，可以验证transform可以正常工作，至此我们自己创建数据集的基本用法结束了，遇到更复杂的问题根据基本用法见招拆招即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
