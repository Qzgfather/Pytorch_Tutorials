{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-2、模型保存\n",
    "当我们的模型训练好以后，需要对模型进行保存，在pytorch中有两种方式对模型进行保存，这一节我们将介绍如何对模型进行保存、加载模型进行预测以及加载模型继续训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.保存、加载整个模型\n",
    "保存了网络的结构和参数，容量比较大，占地方。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     \n",
    "\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                          download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                         download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(     #input_size=(1*28*28)\n",
    "            nn.Conv2d(3, 6, 5, 1, 2), #padding=2保证输入输出尺寸相同\n",
    "            nn.ReLU(),      #input_size=(6*28*28)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#output_size=(6*14*14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),      #input_size=(16*10*10)\n",
    "            nn.MaxPool2d(2, 2)  #output_size=(16*5*5)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(576, 120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用AlexNet进行训练\n",
    "# AlexNet默认的参数太大了如果你不想等的时间太长可以将下面这句解除注释，把model = torchvision.models.alexnet(num_classes=10)注释掉\n",
    "# model = alexnet(num_classes=10)\n",
    "model = LeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs = 1\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train Loss: 22.226948, Train Acc: 0.498400, Eval Loss: 1.278070, Eval Acc: 0.543500\n"
     ]
    }
   ],
   "source": [
    "# 在训练过程中一般一个轮次训练结束后对测试集整体进行测试，获取测试集上的损失和准确率\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train() \n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss.item()是平均损失，平均损失*batch_size=一次训练的损失\n",
    "        train_loss += loss.item() * data.size(0)  \n",
    "        _, pred = output.max(1)\n",
    "        # 计算一个批次对了几个\n",
    "        num_correct = (pred == target).sum().item()\n",
    "        # 计算准确率=对的个数/批次大小\n",
    "        acc = num_correct / data.shape[0]\n",
    "        train_acc += acc\n",
    "    # 统计一个轮次中平均损失与平均准确率\n",
    "#     losses.append(train_loss / len(trainloader.dataset))\n",
    "    acces.append(train_acc / len(trainloader))\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    model.eval() # 将模型改为预测模式\n",
    "    for im, label in testloader:\n",
    "        out = model(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 记录误差\n",
    "        eval_loss += loss.item()\n",
    "        # 记录准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        eval_acc += acc\n",
    "    # 上面的代码对全部的测试集进行测试，下面两行就是讲这一次的整个测试集的损失和准确率存在列表里,如果进行可视化可能会用到\n",
    "#     eval_losses.append(eval_loss / len(testloader.dataset))\n",
    "    eval_acces.append(eval_acc / len(testloader))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}, Eval Loss: {:.6f}, Eval Acc: {:.6f}'\n",
    "          .format(e, train_loss / len(trainloader), train_acc / len(trainloader), \n",
    "                     eval_loss / len(testloader), eval_acc / len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存\n",
    "torch.save(model, './models/LeNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0179, -1.4930,  1.2258,  0.4460,  1.0655, -0.0189,  0.9023, -1.0236,\n",
       "         -0.1456, -1.2768]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型加载\n",
    "# 你可以只加载模型不需要网络的代码\n",
    "model_ = torch.load( './models/LeNet.pth')\n",
    "# 转成测试模式进行计算，如果想继续训练转换成train模式即可\n",
    "model_.eval()\n",
    "test_data = torch.rand((1, 3, 32, 32))\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想保存模型参数，有一个概念很重要，就是模型的状态字典：model.state_dict()，在PyTorch中，torch.nn.Module模型中可学习的参数（即重量和偏置）都包含在模型参数中（使用model.parameters()访问）。state_dict 是个将每层参数映射到对应的参数张量的python字典对象(OrderedDict)。注意:state_dict的条目仅包括带有可学习参数的层(卷积层，线性层等)和registered buffers(BN层的mean等)。优化器对象（torch.optim）也有state_dict ，它包含有关该优化器状态信息，以及所使用的超参数。由于state_dict对象是OrderedDict，它们可以方便地保存，更新，修改和恢复，方便PyTorch模型和优化器添加了大量的模块化。\n",
    "\n",
    "参考：https://blog.csdn.net/sinat_24899403/article/details/102806957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保存参数\n",
    "torch.save(model.state_dict(), './models/LeNet_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.0.weight', 'conv1.0.bias', 'conv2.0.weight', 'conv2.0.bias', 'fc1.0.weight', 'fc1.0.bias', 'fc2.0.weight', 'fc2.0.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果加载只有参数的文佳 首先你要有网络的代码\n",
    "PATH =  './models/LeNet_dict.pth'\n",
    "model_dict = LeNet() \n",
    "print(torch.load(PATH).keys())\n",
    "# 可以看到输出了一些权重值\n",
    "model_dict.load_state_dict(torch.load(PATH))\n",
    "model_dict.eval()\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意一下在使用GPU和CPU混合训练或者跨设备的时候，注意转换细节，看看报错信息，比如使用CPU加载GPU模型文件的时候torch.load函数要添加map_location='cpu'参数。还有更复杂的问题到时候百度就完事了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
